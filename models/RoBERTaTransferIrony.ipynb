{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acbff53c-ccea-4f26-b901-24e589aee6f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>If that's true, then Freedom of Speech is doom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Neener neener - is it time to go in from the p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Just like the plastic gun fear, the armour pie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>So geology is a religion because we weren't he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>Well done Monty. Mark that up as your first ev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6515</th>\n",
       "      <td>1</td>\n",
       "      <td>6516</td>\n",
       "      <td>depends on when the baby bird died.   run alon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6516</th>\n",
       "      <td>1</td>\n",
       "      <td>6517</td>\n",
       "      <td>ok, sheesh, to clarify, women who arent aborti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6517</th>\n",
       "      <td>1</td>\n",
       "      <td>6518</td>\n",
       "      <td>so..   eh??   hows this sound?   will it fly w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6518</th>\n",
       "      <td>1</td>\n",
       "      <td>6519</td>\n",
       "      <td>I think we should put to a vote, the right of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6519</th>\n",
       "      <td>1</td>\n",
       "      <td>6520</td>\n",
       "      <td>You have a blob of tissue in your \"but(sic)\"? ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6520 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     class    id                                               text\n",
       "0        0     1  If that's true, then Freedom of Speech is doom...\n",
       "1        0     2  Neener neener - is it time to go in from the p...\n",
       "2        0     3  Just like the plastic gun fear, the armour pie...\n",
       "3        0     4  So geology is a religion because we weren't he...\n",
       "4        0     5  Well done Monty. Mark that up as your first ev...\n",
       "...    ...   ...                                                ...\n",
       "6515     1  6516  depends on when the baby bird died.   run alon...\n",
       "6516     1  6517  ok, sheesh, to clarify, women who arent aborti...\n",
       "6517     1  6518  so..   eh??   hows this sound?   will it fly w...\n",
       "6518     1  6519  I think we should put to a vote, the right of ...\n",
       "6519     1  6520  You have a blob of tissue in your \"but(sic)\"? ...\n",
       "\n",
       "[6520 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(r'gen_dataset.csv')\n",
    "text_data = list(df[\"text\"])\n",
    "class_data = df[\"class\"].to_numpy()\n",
    "for i in range(len(class_data)):\n",
    "    if class_data[i]=='notsarc':\n",
    "        class_data[i]=0\n",
    "    else:\n",
    "        class_data[i]=1\n",
    "class_data = list(class_data)\n",
    "train_sarc_text_data = text_data[:int(len(text_data) * 0.8)]\n",
    "test_sarc_text_data = text_data[int(len(text_data) * 0.8):]\n",
    "\n",
    "train_sarc_class_data = class_data[:int(len(class_data) * 0.8)]\n",
    "test_sarc_class_data = class_data[int(len(class_data) * 0.8):]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "179be15e-d9ff-43d1-83bc-0b5cb6cda765",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I suspect atheists are projecting their desire...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It's funny how the arguments the shills are ma...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>We are truly following the patterns of how the...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>air pressure dropping as altitude goes higher....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Absolutely.  I think we'd be hard pressed to f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1944</th>\n",
       "      <td>This is an interesting point.  There are no sh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1945</th>\n",
       "      <td>Maybe you mean how we are to respond to the go...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1946</th>\n",
       "      <td>[... What? ](http://tinyurl. com/kw5cpxz)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1947</th>\n",
       "      <td>Does anybody remember during one of the debate...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1948</th>\n",
       "      <td>The pope is meeting a cruel dictator.   Likely...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1949 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           comment_text  label\n",
       "0     I suspect atheists are projecting their desire...      0\n",
       "1     It's funny how the arguments the shills are ma...      0\n",
       "2     We are truly following the patterns of how the...      0\n",
       "3     air pressure dropping as altitude goes higher....      0\n",
       "4     Absolutely.  I think we'd be hard pressed to f...      0\n",
       "...                                                 ...    ...\n",
       "1944  This is an interesting point.  There are no sh...      0\n",
       "1945  Maybe you mean how we are to respond to the go...      0\n",
       "1946          [... What? ](http://tinyurl. com/kw5cpxz)      0\n",
       "1947  Does anybody remember during one of the debate...      0\n",
       "1948  The pope is meeting a cruel dictator.   Likely...      1\n",
       "\n",
       "[1949 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r'irony.csv')\n",
    "text_data1 = list(df[\"comment_text\"])\n",
    "class_data1 = df[\"label\"].to_numpy()\n",
    "for i in range(len(class_data1)):\n",
    "    if class_data1[i]==1:\n",
    "        class_data1[i]=1\n",
    "    else:\n",
    "        class_data1[i]=0\n",
    "class_data1 = list(class_data1)    \n",
    "train_irony_text_data = text_data1[:int(len(text_data1) * 0.8)]\n",
    "test_irony_text_data = text_data1[int(len(text_data1) * 0.8):]\n",
    "\n",
    "train_irony_class_data = class_data1[:int(len(class_data1) * 0.8)]\n",
    "test_irony_class_data = class_data1[int(len(class_data1) * 0.8):]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b56b027-6f26-4286-85b7-0f1a60d2709b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1\n",
      "-----\n",
      "Train Loss:  0.6053553701211245\tTest Loss:  0.5648607277140325\n",
      "Train Accuracy:  71.51098902408893% \tTest Accuracy:  74.40476186421454% \n",
      "Epoch  2\n",
      "-----\n",
      "Train Loss:  0.5858200664703662\tTest Loss:  0.5631560102409247\n",
      "Train Accuracy:  71.89560440870432% \tTest Accuracy:  74.48979591836735% \n",
      "Epoch  3\n",
      "-----\n",
      "Train Loss:  0.5894768145604011\tTest Loss:  0.5830578232298091\n",
      "Train Accuracy:  71.78571429008093% \tTest Accuracy:  74.74489795918367% \n",
      "Epoch  4\n",
      "-----\n",
      "Train Loss:  0.5713978755168426\tTest Loss:  0.5541292775650414\n",
      "Train Accuracy:  72.06959707614703% \tTest Accuracy:  74.14965988421926% \n",
      "Epoch  5\n",
      "-----\n",
      "Train Loss:  0.5828175121392959\tTest Loss:  0.5621761655320927\n",
      "Train Accuracy:  71.7948717948718% \tTest Accuracy:  74.31972793170384% \n",
      "Epoch  6\n",
      "-----\n",
      "Train Loss:  0.5672910888225604\tTest Loss:  0.5464285298877832\n",
      "Train Accuracy:  71.92307692307692% \tTest Accuracy:  74.31972793170384% \n",
      "Epoch  7\n",
      "-----\n",
      "Train Loss:  0.5434634366096595\tTest Loss:  0.5195926266665362\n",
      "Train Accuracy:  72.34432235742226% \tTest Accuracy:  74.31972793170384% \n",
      "Epoch  8\n",
      "-----\n",
      "Train Loss:  0.4924819509570415\tTest Loss:  0.5220614203384945\n",
      "Train Accuracy:  75.7051282051282% \tTest Accuracy:  76.44557819074514% \n",
      "Epoch  9\n",
      "-----\n",
      "Train Loss:  0.4233608060158216\tTest Loss:  0.5664552504919014\n",
      "Train Accuracy:  81.5201465288798% \tTest Accuracy:  74.8299320133365% \n",
      "Epoch  10\n",
      "-----\n",
      "Train Loss:  0.3082865379368648\tTest Loss:  0.7388445534268204\n",
      "Train Accuracy:  87.49084249520914% \tTest Accuracy:  72.87414961931657% \n",
      "Epoch  11\n",
      "-----\n",
      "Train Loss:  0.2034074610600678\tTest Loss:  0.7486759925983391\n",
      "Train Accuracy:  92.75641025641026% \tTest Accuracy:  70.66326530612244% \n",
      "Epoch  12\n",
      "-----\n",
      "Train Loss:  0.14299042991075953\tTest Loss:  0.8327156471430647\n",
      "Train Accuracy:  95.11904762341426% \tTest Accuracy:  70.66326530612244% \n",
      "Epoch  13\n",
      "-----\n",
      "Train Loss:  0.12482049788754337\tTest Loss:  0.9823947664046165\n",
      "Train Accuracy:  96.47435897435898% \tTest Accuracy:  70.40816326530613% \n",
      "Epoch  14\n",
      "-----\n",
      "Train Loss:  0.08802144241292412\tTest Loss:  1.0329809686910285\n",
      "Train Accuracy:  96.66666666666667% \tTest Accuracy:  73.38435376177029% \n",
      "Epoch  15\n",
      "-----\n",
      "Train Loss:  0.036873903825210455\tTest Loss:  1.2441933723166585\n",
      "Train Accuracy:  98.91025641025641% \tTest Accuracy:  72.10884349686759% \n",
      "Epoch  16\n",
      "-----\n",
      "Train Loss:  0.026873711729869965\tTest Loss:  1.5189766160982223\n",
      "Train Accuracy:  99.16666666666667% \tTest Accuracy:  72.27891160517322% \n",
      "Epoch  17\n",
      "-----\n",
      "Train Loss:  0.024368901795241982\tTest Loss:  1.468975649994551\n",
      "Train Accuracy:  98.84615384615385% \tTest Accuracy:  70.57823125196963% \n",
      "Epoch  18\n",
      "-----\n",
      "Train Loss:  0.0376977145575578\tTest Loss:  1.6007535736301048\n",
      "Train Accuracy:  99.1025641025641% \tTest Accuracy:  71.68367346938776% \n",
      "Epoch  19\n",
      "-----\n",
      "Train Loss:  0.046668238943153154\tTest Loss:  1.348445660907452\n",
      "Train Accuracy:  98.46153846153847% \tTest Accuracy:  70.40816326530613% \n",
      "Epoch  20\n",
      "-----\n",
      "Train Loss:  0.021879694765308298\tTest Loss:  1.6204078717736945\n",
      "Train Accuracy:  99.35897435897436% \tTest Accuracy:  71.68367346938776% \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torchmetrics.classification import BinaryAccuracy\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Load the pre-trained BERT model and tokenizer\n",
    "model = AutoModel.from_pretrained('roberta-base')\n",
    "model.to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "# Define the classification model\n",
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, bert_model, num_classes):\n",
    "        super(BertClassifier, self).__init__()\n",
    "        self.bert = bert_model\n",
    "        self.fc = nn.Linear(768, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs[1]\n",
    "        logits = self.fc(pooled_output)\n",
    "        return logits\n",
    "\n",
    "\n",
    "\n",
    "# Tokenize the text data\n",
    "train_encodings = tokenizer(list(train_irony_text_data), padding=True, truncation=True, max_length=128)\n",
    "test_encodings = tokenizer(list(test_irony_text_data), padding=True, truncation=True, max_length=128)\n",
    "\n",
    "# Convert the data into PyTorch tensors\n",
    "train_dataset = torch.utils.data.TensorDataset(\n",
    "    torch.tensor(train_encodings['input_ids']),\n",
    "    torch.tensor(train_encodings['attention_mask']),\n",
    "    torch.tensor(list(train_irony_class_data))\n",
    ")\n",
    "\n",
    "test_dataset = torch.utils.data.TensorDataset(\n",
    "    torch.tensor(test_encodings['input_ids']),\n",
    "    torch.tensor(test_encodings['attention_mask']),\n",
    "    torch.tensor(list(test_irony_class_data))\n",
    ")\n",
    "\n",
    "\n",
    "# Define the training parameters\n",
    "classifier = BertClassifier(model, num_classes=2)\n",
    "classifier.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=2e-5)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=8, shuffle=True)\n",
    "metric = BinaryAccuracy().to(device)\n",
    "\n",
    "\n",
    "# Train the BERT classifier\n",
    "classifier.train()\n",
    "for epoch in range(20):\n",
    "    print(\"Epoch \", epoch+1)\n",
    "    train_losses = []\n",
    "    train_acc = []\n",
    "    test_losses = []\n",
    "    test_acc = []\n",
    "    for idx, batch in enumerate(train_loader):\n",
    "        # batch.to(device)\n",
    "        if(idx % 40 == 0):\n",
    "            print(\"-\", end=\"\")\n",
    "        optimizer.zero_grad()\n",
    "        outputs = classifier(input_ids=batch[0].to(device), attention_mask=batch[1].to(device))\n",
    "        loss = criterion(outputs, batch[2].to(device))\n",
    "        loss.backward()\n",
    "        train_losses.append(loss.cpu().detach().numpy())\n",
    "        optimizer.step()\n",
    "        op = torch.argmax(outputs, axis = 1)\n",
    "        train_acc.append(metric(op, batch[2].to(device)).cpu().detach().numpy())\n",
    "        \n",
    "    for idx, batch in enumerate(test_loader):\n",
    "        # batch.to(device)\n",
    "        outputs = classifier(input_ids=batch[0].to(device), attention_mask=batch[1].to(device))\n",
    "        loss = criterion(outputs, batch[2].to(device))\n",
    "        test_losses.append(loss.cpu().detach().numpy())\n",
    "        op = torch.argmax(outputs, axis = 1)\n",
    "        test_acc.append(metric(op, batch[2].to(device)).cpu().detach().numpy())\n",
    "        \n",
    "        \n",
    "    print()\n",
    "    print(\"Train Loss: \",sum(train_losses) / len(train_losses), end = \"\\t\")\n",
    "    print(\"Test Loss: \",sum(test_losses) / len(test_losses), end = \"\\n\")   \n",
    "    print(\"Train Accuracy: \",(sum(train_acc) / len(train_acc)) * 100, end = \"% \\t\")\n",
    "    print(\"Test Accuracy: \",(sum(test_acc) / len(test_acc)) * 100, end = \"% \\n\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d54c6def-01e2-4ef4-9179-354abd5fb53b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1\n",
      "-----------------\n",
      "Train Loss:  0.48625644518096756\tTest Loss:  0.4104584828476233\n",
      "Train Accuracy:  77.74156441717791% \tTest Accuracy:  82.20858895705521% \n",
      "\n",
      "Epoch  2\n",
      "-----------------\n",
      "Train Loss:  0.36830554340903193\tTest Loss:  0.38820645800488857\n",
      "Train Accuracy:  83.30138036809815% \tTest Accuracy:  82.43865030674846% \n",
      "\n",
      "Epoch  3\n",
      "-----------------\n",
      "Train Loss:  0.273532012036853\tTest Loss:  0.4827343041157247\n",
      "Train Accuracy:  88.66947852760735% \tTest Accuracy:  81.59509202453987% \n",
      "\n",
      "Epoch  4\n",
      "-----------------\n",
      "Train Loss:  0.1987866739809593\tTest Loss:  0.42800919055664466\n",
      "Train Accuracy:  92.23542944785275% \tTest Accuracy:  81.44171779141104% \n",
      "\n",
      "Epoch  5\n",
      "-----------------\n",
      "Train Loss:  0.1288915624460127\tTest Loss:  0.5943102032614678\n",
      "Train Accuracy:  95.43711656441718% \tTest Accuracy:  78.45092024539878% \n",
      "\n",
      "Epoch  6\n",
      "-----------------\n",
      "Train Loss:  0.0854954016771656\tTest Loss:  0.6777588250986843\n",
      "Train Accuracy:  96.66411042944786% \tTest Accuracy:  81.6717791411043% \n",
      "\n",
      "Epoch  7\n",
      "-----------------\n",
      "Train Loss:  0.06087912607981261\tTest Loss:  0.6375154403204019\n",
      "Train Accuracy:  97.87193251533742% \tTest Accuracy:  81.21165644171779% \n",
      "\n",
      "Epoch  8\n",
      "-----------------\n",
      "Train Loss:  0.0553373173494774\tTest Loss:  0.8191462513162937\n",
      "Train Accuracy:  98.15950920245399% \tTest Accuracy:  80.59815950920245% \n",
      "\n",
      "Epoch  9\n",
      "-----------------\n",
      "Train Loss:  0.04854146129553645\tTest Loss:  0.6710157004225016\n",
      "Train Accuracy:  98.56211656441718% \tTest Accuracy:  83.0521472392638% \n",
      "\n",
      "Epoch  10\n",
      "-----------------\n",
      "Train Loss:  0.03620427699045934\tTest Loss:  0.7688406139063689\n",
      "Train Accuracy:  98.75383435582822% \tTest Accuracy:  80.44478527607362% \n",
      "\n",
      "Epoch  11\n",
      "-----------------\n",
      "Train Loss:  0.035030119826954156\tTest Loss:  0.7779880972219163\n",
      "Train Accuracy:  99.04141104294479% \tTest Accuracy:  81.21165644171779% \n",
      "\n",
      "Epoch  12\n",
      "-----------------\n",
      "Train Loss:  0.0314743179342091\tTest Loss:  0.8332560660947469\n",
      "Train Accuracy:  99.04141104294479% \tTest Accuracy:  81.21165644171779% \n",
      "\n",
      "Epoch  13\n",
      "-----------------\n",
      "Train Loss:  0.03821685010358581\tTest Loss:  0.8469609139962807\n",
      "Train Accuracy:  98.77300613496932% \tTest Accuracy:  81.6717791411043% \n",
      "\n",
      "Epoch  14\n",
      "-----------------\n",
      "Train Loss:  0.03701537825101829\tTest Loss:  0.6286488373383903\n",
      "Train Accuracy:  98.73466257668711% \tTest Accuracy:  82.1319018404908% \n",
      "\n",
      "Epoch  15\n",
      "-----------------\n",
      "Train Loss:  0.02189762062459907\tTest Loss:  0.986554516854866\n",
      "Train Accuracy:  99.27147239263803% \tTest Accuracy:  81.97852760736197% \n",
      "\n",
      "Epoch  16\n",
      "-----------------\n",
      "Train Loss:  0.024893817057927887\tTest Loss:  1.064126611617251\n",
      "Train Accuracy:  99.32898773006134% \tTest Accuracy:  82.43865030674846% \n",
      "\n",
      "Epoch  17\n",
      "-----------------\n",
      "Train Loss:  0.031052816761321027\tTest Loss:  0.5350089796070306\n",
      "Train Accuracy:  99.04141104294479% \tTest Accuracy:  81.97852760736197% \n",
      "\n",
      "Epoch  18\n",
      "-----------------\n",
      "Train Loss:  0.02837278632140194\tTest Loss:  0.8958771533839494\n",
      "Train Accuracy:  99.19478527607362% \tTest Accuracy:  82.89877300613497% \n",
      "\n",
      "Epoch  19\n",
      "-----------------\n",
      "Train Loss:  0.02387828994799474\tTest Loss:  1.131739814670048\n",
      "Train Accuracy:  99.23312883435584% \tTest Accuracy:  80.21472392638037% \n",
      "\n",
      "Epoch  20\n",
      "-----------------\n",
      "Train Loss:  0.026924060016443405\tTest Loss:  0.8493830183330935\n",
      "Train Accuracy:  99.19478527607362% \tTest Accuracy:  81.28834355828221% \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the training data\n",
    "# train_texts = [\"Example sentence 1\", \"Example sentence 2\", ...]\n",
    "# train_labels = [0, 1, ...]\n",
    "train_encodings = tokenizer(list(train_sarc_text_data), padding=True, truncation=True, max_length=128)\n",
    "test_encodings = tokenizer(list(test_sarc_text_data), padding=True, truncation=True, max_length=128)\n",
    "\n",
    "# Convert the data into PyTorch tensors\n",
    "train_dataset = torch.utils.data.TensorDataset(\n",
    "    torch.tensor(train_encodings['input_ids']),\n",
    "    torch.tensor(train_encodings['attention_mask']),\n",
    "    torch.tensor(list(train_sarc_class_data))\n",
    ")\n",
    "\n",
    "test_dataset = torch.utils.data.TensorDataset(\n",
    "    torch.tensor(test_encodings['input_ids']),\n",
    "    torch.tensor(test_encodings['attention_mask']),\n",
    "    torch.tensor(list(test_sarc_class_data))\n",
    ")\n",
    "\n",
    "\n",
    "# Define the training parameters\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=8, shuffle=True)\n",
    "metric = BinaryAccuracy().to(device)\n",
    "\n",
    "\n",
    "# Train the BERT classifier\n",
    "classifier.train()\n",
    "for epoch in range(20):\n",
    "    print(\"Epoch \", epoch+1)\n",
    "    train_losses = []\n",
    "    train_acc = []\n",
    "    test_losses = []\n",
    "    test_acc = []\n",
    "    for idx, batch in enumerate(train_loader):\n",
    "        # batch.to(device)\n",
    "        if(idx % 40 == 0):\n",
    "            print(\"-\", end=\"\")\n",
    "        optimizer.zero_grad()\n",
    "        outputs = classifier(input_ids=batch[0].to(device), attention_mask=batch[1].to(device))\n",
    "        loss = criterion(outputs, batch[2].to(device))\n",
    "        loss.backward()\n",
    "        train_losses.append(loss.cpu().detach().numpy())\n",
    "        optimizer.step()\n",
    "        op = torch.argmax(outputs, axis = 1)\n",
    "        train_acc.append(metric(op, batch[2].to(device)).cpu().detach().numpy())\n",
    "        \n",
    "    for idx, batch in enumerate(test_loader):\n",
    "        # batch.to(device)\n",
    "        outputs = classifier(input_ids=batch[0].to(device), attention_mask=batch[1].to(device))\n",
    "        loss = criterion(outputs, batch[2].to(device))\n",
    "        test_losses.append(loss.cpu().detach().numpy())\n",
    "        op = torch.argmax(outputs, axis = 1)\n",
    "        test_acc.append(metric(op, batch[2].to(device)).cpu().detach().numpy())\n",
    "        \n",
    "        \n",
    "    print()\n",
    "    print(\"Train Loss: \",sum(train_losses) / len(train_losses), end = \"\\t\")\n",
    "    print(\"Test Loss: \",sum(test_losses) / len(test_losses), end = \"\\n\")   \n",
    "    print(\"Train Accuracy: \",(sum(train_acc) / len(train_acc)) * 100, end = \"% \\t\")\n",
    "    print(\"Test Accuracy: \",(sum(test_acc) / len(test_acc)) * 100, end = \"% \\n\")   \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22ca1506-6955-43f8-b51a-7e0d5e838de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(classifier, \"RoBERTaTransferIrony.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d948a6df-3897-407f-a0bf-0c5c89816d44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Chintan",
   "language": "python",
   "name": "chintan"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
