{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acbff53c-ccea-4f26-b901-24e589aee6f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>If that's true, then Freedom of Speech is doom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Neener neener - is it time to go in from the p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Just like the plastic gun fear, the armour pie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>So geology is a religion because we weren't he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>Well done Monty. Mark that up as your first ev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6515</th>\n",
       "      <td>1</td>\n",
       "      <td>6516</td>\n",
       "      <td>depends on when the baby bird died.   run alon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6516</th>\n",
       "      <td>1</td>\n",
       "      <td>6517</td>\n",
       "      <td>ok, sheesh, to clarify, women who arent aborti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6517</th>\n",
       "      <td>1</td>\n",
       "      <td>6518</td>\n",
       "      <td>so..   eh??   hows this sound?   will it fly w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6518</th>\n",
       "      <td>1</td>\n",
       "      <td>6519</td>\n",
       "      <td>I think we should put to a vote, the right of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6519</th>\n",
       "      <td>1</td>\n",
       "      <td>6520</td>\n",
       "      <td>You have a blob of tissue in your \"but(sic)\"? ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6520 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     class    id                                               text\n",
       "0        0     1  If that's true, then Freedom of Speech is doom...\n",
       "1        0     2  Neener neener - is it time to go in from the p...\n",
       "2        0     3  Just like the plastic gun fear, the armour pie...\n",
       "3        0     4  So geology is a religion because we weren't he...\n",
       "4        0     5  Well done Monty. Mark that up as your first ev...\n",
       "...    ...   ...                                                ...\n",
       "6515     1  6516  depends on when the baby bird died.   run alon...\n",
       "6516     1  6517  ok, sheesh, to clarify, women who arent aborti...\n",
       "6517     1  6518  so..   eh??   hows this sound?   will it fly w...\n",
       "6518     1  6519  I think we should put to a vote, the right of ...\n",
       "6519     1  6520  You have a blob of tissue in your \"but(sic)\"? ...\n",
       "\n",
       "[6520 rows x 3 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(r'gen_dataset.csv')\n",
    "text_data = list(df[\"text\"])\n",
    "class_data = df[\"class\"].to_numpy()\n",
    "for i in range(len(class_data)):\n",
    "    if class_data[i]=='notsarc':\n",
    "        class_data[i]=0\n",
    "    else:\n",
    "        class_data[i]=1\n",
    "class_data = list(class_data)\n",
    "train_sarc_text_data = text_data[:int(len(text_data) * 0.8)]\n",
    "test_sarc_text_data = text_data[int(len(text_data) * 0.8):]\n",
    "\n",
    "train_sarc_class_data = class_data[:int(len(class_data) * 0.8)]\n",
    "test_sarc_class_data = class_data[int(len(class_data) * 0.8):]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b56b027-6f26-4286-85b7-0f1a60d2709b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from torchmetrics.classification import BinaryAccuracy\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Load the pre-trained BERT model and tokenizer\n",
    "model = AutoModel.from_pretrained('roberta-base')\n",
    "model.to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "# Define the classification model\n",
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, bert_model, num_classes):\n",
    "        super(BertClassifier, self).__init__()\n",
    "        self.bert = bert_model\n",
    "        self.fc = nn.Linear(768, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs[1]\n",
    "        logits = self.fc(pooled_output)\n",
    "        return logits\n",
    "\n",
    "\n",
    "# Define the training parameters\n",
    "classifier = BertClassifier(model, num_classes=2)\n",
    "classifier.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=2e-5)\n",
    "metric = BinaryAccuracy().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d54c6def-01e2-4ef4-9179-354abd5fb53b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1\n",
      "-----\n",
      "Train Loss:  0.4648675854586385\tTest Loss:  0.385839730137732\n",
      "Train Accuracy:  76.61042944785275% \tTest Accuracy:  80.9451219512195% \n",
      "\n",
      "Epoch  2\n",
      "-----\n",
      "Train Loss:  0.3300090053695843\tTest Loss:  0.44386409368456864\n",
      "Train Accuracy:  84.91180981595092% \tTest Accuracy:  81.85975609756098% \n",
      "\n",
      "Epoch  3\n",
      "-----\n",
      "Train Loss:  0.24883697449338216\tTest Loss:  0.3579204180618612\n",
      "Train Accuracy:  89.99233128834356% \tTest Accuracy:  84.90853658536585% \n",
      "\n",
      "Epoch  4\n",
      "-----\n",
      "Train Loss:  0.15773754467490625\tTest Loss:  0.5053636497113763\n",
      "Train Accuracy:  93.90337423312883% \tTest Accuracy:  83.3079268292683% \n",
      "\n",
      "Epoch  5\n",
      "-----\n",
      "Train Loss:  0.09112624582074628\tTest Loss:  0.5291093924968708\n",
      "Train Accuracy:  96.54907975460122% \tTest Accuracy:  84.01930899154848% \n",
      "\n",
      "Epoch  6\n",
      "-----\n",
      "Train Loss:  0.05496251817072736\tTest Loss:  0.6227445713267094\n",
      "Train Accuracy:  97.89110429447852% \tTest Accuracy:  84.0701219512195% \n",
      "\n",
      "Epoch  7\n",
      "-----\n",
      "Train Loss:  0.040189075130152234\tTest Loss:  0.7401736996522764\n",
      "Train Accuracy:  98.61963190184049% \tTest Accuracy:  82.5965446669881% \n",
      "\n",
      "Epoch  8\n",
      "-----\n",
      "Train Loss:  0.0327949033752998\tTest Loss:  0.6882076172566995\n",
      "Train Accuracy:  98.88803680981594% \tTest Accuracy:  83.9939024390244% \n",
      "\n",
      "Epoch  9\n",
      "-----\n",
      "Train Loss:  0.025527166533698128\tTest Loss:  0.7335305126701913\n",
      "Train Accuracy:  99.1372699386503% \tTest Accuracy:  83.61280487804879% \n",
      "\n",
      "Epoch  10\n",
      "-----\n",
      "Train Loss:  0.02277760253201433\tTest Loss:  0.6295558028831715\n",
      "Train Accuracy:  99.25230061349694% \tTest Accuracy:  83.89227637430517% \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the training data\n",
    "# train_texts = [\"Example sentence 1\", \"Example sentence 2\", ...]\n",
    "# train_labels = [0, 1, ...]\n",
    "\n",
    "# Tokenize the text data\n",
    "train_encodings = tokenizer(list(train_sarc_text_data), padding=True, truncation=True, max_length=128)\n",
    "test_encodings = tokenizer(list(test_sarc_text_data), padding=True, truncation=True, max_length=128)\n",
    "\n",
    "# Convert the data into PyTorch tensors\n",
    "train_dataset = torch.utils.data.TensorDataset(\n",
    "    torch.tensor(train_encodings['input_ids']),\n",
    "    torch.tensor(train_encodings['attention_mask']),\n",
    "    torch.tensor(list(train_sarc_class_data))\n",
    ")\n",
    "\n",
    "test_dataset = torch.utils.data.TensorDataset(\n",
    "    torch.tensor(test_encodings['input_ids']),\n",
    "    torch.tensor(test_encodings['attention_mask']),\n",
    "    torch.tensor(list(test_sarc_class_data))\n",
    ")\n",
    "\n",
    "\n",
    "# Define the training parameters\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=True)\n",
    "metric = BinaryAccuracy().to(device)\n",
    "\n",
    "\n",
    "# Train the BERT classifier\n",
    "classifier.train()\n",
    "for epoch in range(10):\n",
    "    print(\"Epoch \", epoch+1)\n",
    "    train_losses = []\n",
    "    train_acc = []\n",
    "    test_losses = []\n",
    "    test_acc = []\n",
    "    for idx, batch in enumerate(train_loader):\n",
    "        # batch.to(device)\n",
    "        if(idx % 40 == 0):\n",
    "            print(\"-\", end=\"\")\n",
    "        optimizer.zero_grad()\n",
    "        outputs = classifier(input_ids=batch[0].to(device), attention_mask=batch[1].to(device))\n",
    "        loss = criterion(outputs, batch[2].to(device))\n",
    "        loss.backward()\n",
    "        train_losses.append(loss.cpu().detach().numpy())\n",
    "        optimizer.step()\n",
    "        op = torch.argmax(outputs, axis = 1)\n",
    "        train_acc.append(metric(op, batch[2].to(device)).cpu().detach().numpy())\n",
    "        \n",
    "    for idx, batch in enumerate(test_loader):\n",
    "        # batch.to(device)\n",
    "        outputs = classifier(input_ids=batch[0].to(device), attention_mask=batch[1].to(device))\n",
    "        loss = criterion(outputs, batch[2].to(device))\n",
    "        test_losses.append(loss.cpu().detach().numpy())\n",
    "        op = torch.argmax(outputs, axis = 1)\n",
    "        test_acc.append(metric(op, batch[2].to(device)).cpu().detach().numpy())\n",
    "        \n",
    "        \n",
    "    print()\n",
    "    print(\"Train Loss: \",sum(train_losses) / len(train_losses), end = \"\\t\")\n",
    "    print(\"Test Loss: \",sum(test_losses) / len(test_losses), end = \"\\n\")   \n",
    "    print(\"Train Accuracy: \",(sum(train_acc) / len(train_acc)) * 100, end = \"% \\t\")\n",
    "    print(\"Test Accuracy: \",(sum(test_acc) / len(test_acc)) * 100, end = \"% \\n\")   \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22ca1506-6955-43f8-b51a-7e0d5e838de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(classifier, \"RoBERTa.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d948a6df-3897-407f-a0bf-0c5c89816d44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Chintan",
   "language": "python",
   "name": "chintan"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
