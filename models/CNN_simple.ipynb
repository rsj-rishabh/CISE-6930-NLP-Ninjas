{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import gensim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re,string,unicodedata\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from wordcloud import WordCloud,STOPWORDS\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from gensim.utils import simple_preprocess \n",
        "from gensim.models import Phrases, Word2Vec\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "\n",
        "\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yd8D9Zb_vspT",
        "outputId": "ad52be88-75fa-4c2c-f515-a392ce0aaae8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the IMDb dataset from the CSV file\n",
        "imdb_df = pd.read_csv('/imdb_dataset.csv', header=None, names=['review', 'sentiment'])\n",
        "imdb_df[\"sentiment\"] = np.where(imdb_df[\"sentiment\"] == \"positive\", 0, 1)\n",
        "\n",
        "gen_df = pd.read_csv('/GEN-sarc-notsarc.csv', header=None, names=['class', 'text'])\n",
        "gen_df = gen_df.append(pd.read_csv('/HYP-sarc-notsarc.csv'))\n",
        "gen_df = gen_df.append(pd.read_csv('/RQ-sarc-notsarc.csv'))\n",
        "\n",
        "gen_df[\"class\"] = np.where(gen_df[\"class\"] == \"notsarc\", 0, 1)\n",
        "\n",
        "stop = set(stopwords.words('english'))\n",
        "punctuation = list(string.punctuation)\n",
        "stop.update(punctuation)\n",
        "\n",
        "# Preprocess the dataset\n",
        "def strip_html(text):\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    return soup.get_text()\n",
        "\n",
        "#Removing the square brackets\n",
        "def remove_between_square_brackets(text):\n",
        "    return re.sub('\\[[^]]*\\]', '', text)\n",
        "# Removing URL's\n",
        "def remove_between_square_brackets(text):\n",
        "    return re.sub(r'http\\S+', '', text)\n",
        "#Removing the stopwords from text\n",
        "def remove_stopwords(text):\n",
        "    final_text = []\n",
        "    for i in text.split():\n",
        "        if i.strip().lower() not in stop:\n",
        "            final_text.append(i.strip())\n",
        "    return \" \".join(final_text)\n",
        "#Removing the noisy text\n",
        "def denoise_text(text):\n",
        "    text = strip_html(text)\n",
        "    text = remove_between_square_brackets(text)\n",
        "    text = remove_stopwords(text)\n",
        "    return text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KmS_uZMhviOp",
        "outputId": "9ad609dd-3215-47bf-8686-270cc58dda7a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-5c5b39dec5c3>:6: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  gen_df = gen_df.append(pd.read_csv('/HYP-sarc-notsarc.csv'))\n",
            "<ipython-input-2-5c5b39dec5c3>:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  gen_df = gen_df.append(pd.read_csv('/RQ-sarc-notsarc.csv'))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "imdb_df['review']=imdb_df['review'].apply(denoise_text)\n",
        "gen_df['text']=gen_df['text'].apply(denoise_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yrm4JS6hwoqc",
        "outputId": "89826f3c-4e30-40c5-941e-29c1d1e4a760"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-5c5b39dec5c3>:17: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
            "  soup = BeautifulSoup(text, \"html.parser\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words, imdb_words, gen_words = [], [], []\n",
        "for i in imdb_df.review.values:\n",
        "    words.append(i.split())\n",
        "    imdb_words.append(i.split())\n",
        "words=words[1:]\n",
        "\n",
        "for i in gen_df.text.values:\n",
        "    words.append(i.split())\n",
        "    gen_words.append(i.split())"
      ],
      "metadata": {
        "id": "pp7HQIkewzF4"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imdb_wv_model = Word2Vec(words, vector_size=500, window=3, min_count=1, workers=16)"
      ],
      "metadata": {
        "id": "0HwrOy-J29y2"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(imdb_wv_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7RkjUghZ3ORS",
        "outputId": "ee99252c-0562-4e90-a2f2-4d7682c0ac05"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word2Vec<vocab=485482, vector_size=500, alpha=0.025>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iPVUi1Nz7b4W",
        "outputId": "3f4c40b2-83e1-4a72-d4ca-1d7b04f6d1ec"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "59387\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer(485482)\n",
        "tokenizer.fit_on_texts(words)\n",
        "text = tokenizer.texts_to_sequences(words)\n",
        "text = keras.utils.pad_sequences(text, 75)\n",
        "\n",
        "print(text[:2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L67LkbKU3PbY",
        "outputId": "5a4bd383-d5e9-42ae-a2c9-429dcbb3d843"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[   145    166  42448    703     75   1528   4516   2772   7673    703\n",
            "    7962    703 154647   1293   2081     17    355     37     99   3229\n",
            "    1655  11805     43   1401     95    159   1888   1562   1551  15623\n",
            "      79  11649    196   3075   2146   3154   2091  10361 154648   6038\n",
            "   15624   2956 105798   8723  15624    341    483     10    168     95\n",
            "      26  54856    604    823   8723    466   1376  33356    478    366\n",
            "    1051   2583   1376  54857     57  15623     84    247   3879   3815\n",
            "  154649     10   1237   4495   4324]\n",
            " [   201   2519    252   5587     86    369     26 154652    374   6640\n",
            "   45740     79 154653   2650   3700   5609    203      9  21813    913\n",
            "   11806   1916  14464  11483  33357     26    148     57  61470    345\n",
            "    2524   5587   4964    291      3     18  19547    172    389   2519\n",
            "       8    138    302     27  11650   1119   3719   2270    121    190\n",
            "    2027 105800   3857   1095   1080  21183    164   1799  18585    419\n",
            "      64   3416  27819  39661    675   7296   1340 154654 154655  39662\n",
            "      59  61471   1803     26   1459]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = imdb_df[\"sentiment\"]"
      ],
      "metadata": {
        "id": "QJA50BEmCoWh"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "txt = tokenizer.texts_to_sequences(imdb_words)\n",
        "txt = keras.utils.pad_sequences(txt, 75)"
      ],
      "metadata": {
        "id": "RacOsXm2hZ1r"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(np.array(txt), np.array(y), train_size=0.8, stratify=y)"
      ],
      "metadata": {
        "id": "Qqh1HMzg4IP0"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.Sequential([\n",
        "\n",
        "    # Embedding layer\n",
        "    keras.layers.Embedding(input_dim=len(imdb_wv_model.wv.key_to_index),\n",
        "                            output_dim=500,\n",
        "                            input_length=75,\n",
        "                            weights=[imdb_wv_model.wv.vectors],\n",
        "                            trainable=True),\n",
        "    keras.layers.Conv1D(50, 3, activation='relu', padding='same', strides=1),\n",
        "    keras.layers.MaxPool1D(2),\n",
        "    keras.layers.Flatten(),\n",
        "    keras.layers.Dense(10),\n",
        "    keras.layers.Activation('relu'),\n",
        "    keras.layers.Dense(1),\n",
        "    keras.layers.Activation('sigmoid')\n",
        "])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', keras.metrics.Precision(), keras.metrics.Recall()])\n",
        "\n",
        "model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=10, batch_size=64)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSvtr3TQ14YU",
        "outputId": "2e1fadab-4fc2-473f-dae0-26a00748dac2"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_3 (Embedding)     (None, 75, 500)           242741000 \n",
            "                                                                 \n",
            " conv1d_13 (Conv1D)          (None, 75, 50)            75050     \n",
            "                                                                 \n",
            " max_pooling1d_5 (MaxPooling  (None, 37, 50)           0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 1850)              0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 10)                18510     \n",
            "                                                                 \n",
            " activation_4 (Activation)   (None, 10)                0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 1)                 11        \n",
            "                                                                 \n",
            " activation_5 (Activation)   (None, 1)                 0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 242,834,571\n",
            "Trainable params: 242,834,571\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "625/625 [==============================] - 79s 123ms/step - loss: 0.5154 - accuracy: 0.7071 - precision_2: 0.6777 - recall_2: 0.7901 - val_loss: 0.3168 - val_accuracy: 0.8644 - val_precision_2: 0.8820 - val_recall_2: 0.8414\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 48s 76ms/step - loss: 0.1833 - accuracy: 0.9308 - precision_2: 0.9335 - recall_2: 0.9276 - val_loss: 0.3071 - val_accuracy: 0.8715 - val_precision_2: 0.8764 - val_recall_2: 0.8650\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 40s 64ms/step - loss: 0.0239 - accuracy: 0.9936 - precision_2: 0.9945 - recall_2: 0.9926 - val_loss: 0.4207 - val_accuracy: 0.8638 - val_precision_2: 0.8714 - val_recall_2: 0.8536\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 40s 64ms/step - loss: 0.0021 - accuracy: 0.9997 - precision_2: 0.9998 - recall_2: 0.9995 - val_loss: 0.5135 - val_accuracy: 0.8627 - val_precision_2: 0.8595 - val_recall_2: 0.8672\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 40s 63ms/step - loss: 3.9171e-04 - accuracy: 1.0000 - precision_2: 1.0000 - recall_2: 0.9999 - val_loss: 0.7053 - val_accuracy: 0.8434 - val_precision_2: 0.7920 - val_recall_2: 0.9314\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 39s 62ms/step - loss: 1.2734e-04 - accuracy: 1.0000 - precision_2: 1.0000 - recall_2: 1.0000 - val_loss: 0.6144 - val_accuracy: 0.8622 - val_precision_2: 0.8568 - val_recall_2: 0.8698\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 37s 60ms/step - loss: 3.4717e-05 - accuracy: 1.0000 - precision_2: 1.0000 - recall_2: 1.0000 - val_loss: 0.7202 - val_accuracy: 0.8588 - val_precision_2: 0.8518 - val_recall_2: 0.8688\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 38s 61ms/step - loss: 1.2520e-05 - accuracy: 1.0000 - precision_2: 1.0000 - recall_2: 1.0000 - val_loss: 0.7737 - val_accuracy: 0.8606 - val_precision_2: 0.8565 - val_recall_2: 0.8664\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 39s 62ms/step - loss: 6.5781e-06 - accuracy: 1.0000 - precision_2: 1.0000 - recall_2: 1.0000 - val_loss: 0.8158 - val_accuracy: 0.8607 - val_precision_2: 0.8540 - val_recall_2: 0.8702\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 38s 61ms/step - loss: 3.8366e-06 - accuracy: 1.0000 - precision_2: 1.0000 - recall_2: 1.0000 - val_loss: 0.8528 - val_accuracy: 0.8617 - val_precision_2: 0.8598 - val_recall_2: 0.8644\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f7f32f1dd90>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_recall = 0.8160\n",
        "val_precision = 0.8572\n",
        "val_f1 = (2 * val_precision * val_recall) / (val_precision + val_recall)\n",
        "\n",
        "val_f1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7YcCleAvkJ4s",
        "outputId": "cbdff001-3b26-4d8e-cfae-1c1892773e0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8360927563949319"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "txt2 = tokenizer.texts_to_sequences(gen_words)\n",
        "txt2 = keras.utils.pad_sequences(txt2, 75)\n",
        "\n",
        "y = gen_df[\"class\"]\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(np.array(txt2), np.array(y), train_size=0.8, stratify=y)\n",
        "\n",
        "model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=10)"
      ],
      "metadata": {
        "id": "K-q-Fpsl97A0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23a14991-f5e0-4e3e-96da-bb8b35197065"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "235/235 [==============================] - 47s 197ms/step - loss: 0.4818 - accuracy: 0.8398 - precision_2: 0.8478 - recall_2: 0.9884 - val_loss: 0.4013 - val_accuracy: 0.8472 - val_precision_2: 0.8472 - val_recall_2: 1.0000\n",
            "Epoch 2/10\n",
            "235/235 [==============================] - 26s 112ms/step - loss: 0.2622 - accuracy: 0.8688 - precision_2: 0.8703 - recall_2: 0.9932 - val_loss: 0.5286 - val_accuracy: 0.8360 - val_precision_2: 0.8570 - val_recall_2: 0.9679\n",
            "Epoch 3/10\n",
            "235/235 [==============================] - 21s 92ms/step - loss: 0.1140 - accuracy: 0.9543 - precision_2: 0.9655 - recall_2: 0.9811 - val_loss: 0.7867 - val_accuracy: 0.8056 - val_precision_2: 0.8671 - val_recall_2: 0.9101\n",
            "Epoch 4/10\n",
            "235/235 [==============================] - 18s 76ms/step - loss: 0.0489 - accuracy: 0.9832 - precision_2: 0.9923 - recall_2: 0.9879 - val_loss: 0.8801 - val_accuracy: 0.7843 - val_precision_2: 0.8660 - val_recall_2: 0.8818\n",
            "Epoch 5/10\n",
            "235/235 [==============================] - 18s 77ms/step - loss: 0.0391 - accuracy: 0.9860 - precision_2: 0.9945 - recall_2: 0.9890 - val_loss: 0.9097 - val_accuracy: 0.7822 - val_precision_2: 0.8644 - val_recall_2: 0.8812\n",
            "Epoch 6/10\n",
            "235/235 [==============================] - 17s 71ms/step - loss: 0.0328 - accuracy: 0.9875 - precision_2: 0.9953 - recall_2: 0.9899 - val_loss: 0.9307 - val_accuracy: 0.7929 - val_precision_2: 0.8687 - val_recall_2: 0.8900\n",
            "Epoch 7/10\n",
            "235/235 [==============================] - 15s 66ms/step - loss: 0.0301 - accuracy: 0.9861 - precision_2: 0.9932 - recall_2: 0.9904 - val_loss: 0.9441 - val_accuracy: 0.7614 - val_precision_2: 0.8713 - val_recall_2: 0.8429\n",
            "Epoch 8/10\n",
            "235/235 [==============================] - 16s 68ms/step - loss: 0.0274 - accuracy: 0.9871 - precision_2: 0.9953 - recall_2: 0.9895 - val_loss: 0.9747 - val_accuracy: 0.7732 - val_precision_2: 0.8670 - val_recall_2: 0.8649\n",
            "Epoch 9/10\n",
            "235/235 [==============================] - 15s 64ms/step - loss: 0.0239 - accuracy: 0.9867 - precision_2: 0.9934 - recall_2: 0.9909 - val_loss: 1.1230 - val_accuracy: 0.7359 - val_precision_2: 0.8697 - val_recall_2: 0.8096\n",
            "Epoch 10/10\n",
            "235/235 [==============================] - 16s 68ms/step - loss: 0.0232 - accuracy: 0.9859 - precision_2: 0.9935 - recall_2: 0.9898 - val_loss: 1.0404 - val_accuracy: 0.7694 - val_precision_2: 0.8693 - val_recall_2: 0.8567\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f7f32ed3730>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_recall = 0.8693 *100\n",
        "val_precision = 0.8567 *100\n",
        "val_f1 = (2 * val_precision * val_recall) / (val_precision + val_recall)\n",
        "\n",
        "val_f1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iP0PzAn7krA9",
        "outputId": "58753d74-5f82-4680-b464-c70c800fc2c2"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "86.29540092699884"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"/cnn-imdb.h\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pfxrQD7Xl-cc",
        "outputId": "c7d65794-e0da-4cf2-c207-950a1897c276"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gen_wv_model = Word2Vec(gen_words, vector_size=500, window=3, min_count=1, workers=16)"
      ],
      "metadata": {
        "id": "JYfXu-htBH_e"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(gen_wv_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-cf_G0T5BNyd",
        "outputId": "e7c3da17-9aee-4596-a746-6eb243d973a6"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word2Vec<vocab=46908, vector_size=500, alpha=0.025>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizer = Tokenizer(46908)\n",
        "# tokenizer.fit_on_texts(gen_words)\n",
        "txt2 = tokenizer.texts_to_sequences(gen_words)\n",
        "txt2 = keras.utils.pad_sequences(txt2, 75)\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "\n",
        "    keras.layers.Embedding(input_dim=len(imdb_wv_model.wv.key_to_index),\n",
        "                            output_dim=500,\n",
        "                            input_length=75,\n",
        "                            weights=[imdb_wv_model.wv.vectors],\n",
        "                            trainable=True),\n",
        "    keras.layers.Conv1D(50, 3, activation='relu', padding='same', strides=1),\n",
        "    keras.layers.MaxPool1D(2),\n",
        "    keras.layers.Flatten(),\n",
        "    keras.layers.Dense(10),\n",
        "    keras.layers.Activation('relu'),\n",
        "    keras.layers.Dense(1),\n",
        "    keras.layers.Activation('sigmoid')\n",
        "])\n",
        "\n",
        "\n",
        "y = gen_df[\"class\"]\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(np.array(txt2), np.array(y), train_size=0.8, stratify=y, random_state=42)\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', keras.metrics.Precision(), keras.metrics.Recall()])\n",
        "\n",
        "model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=10, batch_size=64)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKkAgrQOF-0I",
        "outputId": "6b3279b9-e64d-4fa4-dc75-b9573e973c9c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "118/118 [==============================] - 30s 236ms/step - loss: 0.4206 - accuracy: 0.8450 - precision_3: 0.8474 - recall_3: 0.9965 - val_loss: 0.4129 - val_accuracy: 0.8472 - val_precision_3: 0.8472 - val_recall_3: 1.0000\n",
            "Epoch 2/10\n",
            "118/118 [==============================] - 21s 181ms/step - loss: 0.3566 - accuracy: 0.8474 - precision_3: 0.8474 - recall_3: 1.0000 - val_loss: 0.4327 - val_accuracy: 0.8472 - val_precision_3: 0.8472 - val_recall_3: 1.0000\n",
            "Epoch 3/10\n",
            "118/118 [==============================] - 16s 140ms/step - loss: 0.2660 - accuracy: 0.8904 - precision_3: 0.8870 - recall_3: 0.9978 - val_loss: 0.4463 - val_accuracy: 0.8365 - val_precision_3: 0.8516 - val_recall_3: 0.9774\n",
            "Epoch 4/10\n",
            "118/118 [==============================] - 14s 118ms/step - loss: 0.1720 - accuracy: 0.9435 - precision_3: 0.9422 - recall_3: 0.9943 - val_loss: 0.5236 - val_accuracy: 0.8403 - val_precision_3: 0.8506 - val_recall_3: 0.9843\n",
            "Epoch 5/10\n",
            "118/118 [==============================] - 13s 114ms/step - loss: 0.1214 - accuracy: 0.9755 - precision_3: 0.9792 - recall_3: 0.9921 - val_loss: 0.5471 - val_accuracy: 0.8291 - val_precision_3: 0.8516 - val_recall_3: 0.9667\n",
            "Epoch 6/10\n",
            "118/118 [==============================] - 12s 102ms/step - loss: 0.1007 - accuracy: 0.9860 - precision_3: 0.9926 - recall_3: 0.9909 - val_loss: 0.5959 - val_accuracy: 0.8243 - val_precision_3: 0.8505 - val_recall_3: 0.9617\n",
            "Epoch 7/10\n",
            "118/118 [==============================] - 11s 91ms/step - loss: 0.0887 - accuracy: 0.9900 - precision_3: 0.9978 - recall_3: 0.9904 - val_loss: 0.6910 - val_accuracy: 0.8301 - val_precision_3: 0.8491 - val_recall_3: 0.9723\n",
            "Epoch 8/10\n",
            "118/118 [==============================] - 10s 81ms/step - loss: 0.0825 - accuracy: 0.9903 - precision_3: 0.9981 - recall_3: 0.9904 - val_loss: 0.5984 - val_accuracy: 0.8104 - val_precision_3: 0.8547 - val_recall_3: 0.9353\n",
            "Epoch 9/10\n",
            "118/118 [==============================] - 10s 86ms/step - loss: 0.0771 - accuracy: 0.9908 - precision_3: 0.9987 - recall_3: 0.9904 - val_loss: 0.7104 - val_accuracy: 0.8296 - val_precision_3: 0.8525 - val_recall_3: 0.9661\n",
            "Epoch 10/10\n",
            "118/118 [==============================] - 9s 78ms/step - loss: 0.0719 - accuracy: 0.9913 - precision_3: 0.9994 - recall_3: 0.9904 - val_loss: 0.7575 - val_accuracy: 0.8285 - val_precision_3: 0.8519 - val_recall_3: 0.9654\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f7f2e3536d0>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_recall = 0.8519 *100\n",
        "val_precision = 0.9654 *100\n",
        "val_f1 = (2 * val_precision * val_recall) / (val_precision + val_recall)\n",
        "\n",
        "val_f1"
      ],
      "metadata": {
        "id": "zzCxsmZkAuoD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42450b14-f2d0-4d61-f391-7adb08c375fe"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "90.51056622461893"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r \"/bilstm.zip\" \"/bert-with-intermediate-task.h\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "llUwZ02NtfpW",
        "outputId": "c380ba45-0100-4957-9157-e8f186db2d37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: bert-with-intermediate-task.h/ (stored 0%)\n",
            "  adding: bert-with-intermediate-task.h/assets/ (stored 0%)\n",
            "  adding: bert-with-intermediate-task.h/keras_metadata.pb (deflated 92%)\n",
            "  adding: bert-with-intermediate-task.h/variables/ (stored 0%)\n",
            "  adding: bert-with-intermediate-task.h/variables/variables.index (deflated 67%)\n",
            "  adding: bert-with-intermediate-task.h/variables/variables.data-00000-of-00001 (deflated 48%)\n",
            "  adding: bert-with-intermediate-task.h/fingerprint.pb (stored 0%)\n",
            "  adding: bert-with-intermediate-task.h/saved_model.pb (deflated 90%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ehjAu4Urt4zE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}