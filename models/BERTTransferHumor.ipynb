{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "acbff53c-ccea-4f26-b901-24e589aee6f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>If that's true, then Freedom of Speech is doom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Neener neener - is it time to go in from the p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Just like the plastic gun fear, the armour pie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>So geology is a religion because we weren't he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>Well done Monty. Mark that up as your first ev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6515</th>\n",
       "      <td>1</td>\n",
       "      <td>6516</td>\n",
       "      <td>depends on when the baby bird died.   run alon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6516</th>\n",
       "      <td>1</td>\n",
       "      <td>6517</td>\n",
       "      <td>ok, sheesh, to clarify, women who arent aborti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6517</th>\n",
       "      <td>1</td>\n",
       "      <td>6518</td>\n",
       "      <td>so..   eh??   hows this sound?   will it fly w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6518</th>\n",
       "      <td>1</td>\n",
       "      <td>6519</td>\n",
       "      <td>I think we should put to a vote, the right of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6519</th>\n",
       "      <td>1</td>\n",
       "      <td>6520</td>\n",
       "      <td>You have a blob of tissue in your \"but(sic)\"? ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6520 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     class    id                                               text\n",
       "0        0     1  If that's true, then Freedom of Speech is doom...\n",
       "1        0     2  Neener neener - is it time to go in from the p...\n",
       "2        0     3  Just like the plastic gun fear, the armour pie...\n",
       "3        0     4  So geology is a religion because we weren't he...\n",
       "4        0     5  Well done Monty. Mark that up as your first ev...\n",
       "...    ...   ...                                                ...\n",
       "6515     1  6516  depends on when the baby bird died.   run alon...\n",
       "6516     1  6517  ok, sheesh, to clarify, women who arent aborti...\n",
       "6517     1  6518  so..   eh??   hows this sound?   will it fly w...\n",
       "6518     1  6519  I think we should put to a vote, the right of ...\n",
       "6519     1  6520  You have a blob of tissue in your \"but(sic)\"? ...\n",
       "\n",
       "[6520 rows x 3 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(r'gen_dataset.csv')\n",
    "text_data = list(df[\"text\"])\n",
    "class_data = df[\"class\"].to_numpy()\n",
    "for i in range(len(class_data)):\n",
    "    if class_data[i]=='notsarc':\n",
    "        class_data[i]=0\n",
    "    else:\n",
    "        class_data[i]=1\n",
    "class_data = list(class_data)\n",
    "train_sarc_text_data = text_data[:int(len(text_data) * 0.8)]\n",
    "test_sarc_text_data = text_data[int(len(text_data) * 0.8):]\n",
    "\n",
    "train_sarc_class_data = class_data[:int(len(class_data) * 0.8)]\n",
    "test_sarc_class_data = class_data[int(len(class_data) * 0.8):]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "179be15e-d9ff-43d1-83bc-0b5cb6cda765",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "df = pd.read_csv(r'humor.csv')\n",
    "text_data1 = list(df[\"text\"])\n",
    "class_data1 = list(df[\"humor\"])\n",
    "for i in range(len(class_data1)):\n",
    "    if class_data1[i]==False:\n",
    "        class_data1[i]=1\n",
    "    else:\n",
    "        class_data1[i]=0\n",
    "# class_data1 = list(class_data1)    \n",
    "train_humor_text_data = text_data1[:int(len(text_data1) * 0.8)]\n",
    "test_humor_text_data = text_data1[int(len(text_data1) * 0.8):]\n",
    "\n",
    "train_humor_class_data = class_data1[:int(len(class_data1) * 0.8)]\n",
    "test_humor_class_data = class_data1[int(len(class_data1) * 0.8):]\n",
    "# df\n",
    "test_irony_class_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4b56b027-6f26-4286-85b7-0f1a60d2709b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1\n",
      "---------------------------------------------------------------\n",
      "Train Loss:  0.053014048401406035\tTest Loss:  0.04784564924915321\n",
      "Train Accuracy:  98.165% \tTest Accuracy:  98.4275% \n",
      "Epoch  2\n",
      "---------------------------------------------------------------\n",
      "Train Loss:  0.019048631024637143\tTest Loss:  0.04763360193041735\n",
      "Train Accuracy:  99.33562500000001% \tTest Accuracy:  98.5275% \n",
      "Epoch  3\n",
      "---------------------------------------------------------------\n",
      "Train Loss:  0.008005155239285524\tTest Loss:  0.05920548996393918\n",
      "Train Accuracy:  99.71% \tTest Accuracy:  98.485% \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torchmetrics.classification import BinaryAccuracy\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Load the pre-trained BERT model and tokenizer\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "model.to(device)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Define the classification model\n",
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, bert_model, num_classes):\n",
    "        super(BertClassifier, self).__init__()\n",
    "        self.bert = bert_model\n",
    "        self.fc = nn.Linear(768, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs[1]\n",
    "        logits = self.fc(pooled_output)\n",
    "        return logits\n",
    "\n",
    "\n",
    "\n",
    "# Tokenize the text data\n",
    "train_encodings = tokenizer(list(train_humor_text_data), padding=True, truncation=True, max_length=128)\n",
    "test_encodings = tokenizer(list(test_humor_text_data), padding=True, truncation=True, max_length=128)\n",
    "\n",
    "# Convert the data into PyTorch tensors\n",
    "train_dataset = torch.utils.data.TensorDataset(\n",
    "    torch.tensor(train_encodings['input_ids']),\n",
    "    torch.tensor(train_encodings['attention_mask']),\n",
    "    torch.tensor(list(train_humor_class_data))\n",
    ")\n",
    "\n",
    "test_dataset = torch.utils.data.TensorDataset(\n",
    "    torch.tensor(test_encodings['input_ids']),\n",
    "    torch.tensor(test_encodings['attention_mask']),\n",
    "    torch.tensor(list(test_humor_class_data))\n",
    ")\n",
    "\n",
    "\n",
    "# Define the training parameters\n",
    "classifier = BertClassifier(model, num_classes=2)\n",
    "classifier.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=2e-5)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=True)\n",
    "metric = BinaryAccuracy().to(device)\n",
    "\n",
    "\n",
    "# Train the BERT classifier\n",
    "classifier.train()\n",
    "for epoch in range(3):\n",
    "    print(\"Epoch \", epoch+1)\n",
    "    train_losses = []\n",
    "    train_acc = []\n",
    "    test_losses = []\n",
    "    test_acc = []\n",
    "    for idx, batch in enumerate(train_loader):\n",
    "        # batch.to(device)\n",
    "        if(idx % 80 == 0):\n",
    "            print(\"-\", end=\"\")\n",
    "        optimizer.zero_grad()\n",
    "        outputs = classifier(input_ids=batch[0].to(device), attention_mask=batch[1].to(device))\n",
    "        loss = criterion(outputs, batch[2].to(device))\n",
    "        loss.backward()\n",
    "        train_losses.append(loss.cpu().detach().numpy())\n",
    "        optimizer.step()\n",
    "        op = torch.argmax(outputs, axis = 1)\n",
    "        train_acc.append(metric(op, batch[2].to(device)).cpu().detach().numpy())\n",
    "        \n",
    "    for idx, batch in enumerate(test_loader):\n",
    "        # batch.to(device)\n",
    "        outputs = classifier(input_ids=batch[0].to(device), attention_mask=batch[1].to(device))\n",
    "        loss = criterion(outputs, batch[2].to(device))\n",
    "        test_losses.append(loss.cpu().detach().numpy())\n",
    "        op = torch.argmax(outputs, axis = 1)\n",
    "        test_acc.append(metric(op, batch[2].to(device)).cpu().detach().numpy())\n",
    "        \n",
    "        \n",
    "    print()\n",
    "    print(\"Train Loss: \",sum(train_losses) / len(train_losses), end = \"\\t\")\n",
    "    print(\"Test Loss: \",sum(test_losses) / len(test_losses), end = \"\\n\")   \n",
    "    print(\"Train Accuracy: \",(sum(train_acc) / len(train_acc)) * 100, end = \"% \\t\")\n",
    "    print(\"Test Accuracy: \",(sum(test_acc) / len(test_acc)) * 100, end = \"% \\n\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d54c6def-01e2-4ef4-9179-354abd5fb53b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1\n",
      "-----\n",
      "Train Loss:  0.6583910535083958\tTest Loss:  0.4879914471289007\n",
      "Train Accuracy:  68.40490797546013% \tTest Accuracy:  76.54979679642653% \n",
      "\n",
      "Epoch  2\n",
      "-----\n",
      "Train Loss:  0.41941974728019693\tTest Loss:  0.45870901171754047\n",
      "Train Accuracy:  81.21165644171779% \tTest Accuracy:  78.53150411349971% \n",
      "\n",
      "Epoch  3\n",
      "-----\n",
      "Train Loss:  0.2504839588847994\tTest Loss:  0.5495520959176668\n",
      "Train Accuracy:  90.70168711656443% \tTest Accuracy:  78.35365853658537% \n",
      "\n",
      "Epoch  4\n",
      "-----\n",
      "Train Loss:  0.10639745414691286\tTest Loss:  0.6523970819827987\n",
      "Train Accuracy:  96.64493865030674% \tTest Accuracy:  78.60772362569483% \n",
      "\n",
      "Epoch  5\n",
      "-----\n",
      "Train Loss:  0.05429991830763139\tTest Loss:  0.7872498297109837\n",
      "Train Accuracy:  98.6388036809816% \tTest Accuracy:  77.36280487804879% \n",
      "\n",
      "Epoch  6\n",
      "-----\n",
      "Train Loss:  0.03752185793827785\tTest Loss:  0.9135208464250332\n",
      "Train Accuracy:  98.81134969325154% \tTest Accuracy:  77.23577240618263% \n",
      "\n",
      "Epoch  7\n",
      "-----\n",
      "Train Loss:  0.01966133077794606\tTest Loss:  1.0317731368832472\n",
      "Train Accuracy:  99.38650306748467% \tTest Accuracy:  76.90548780487805% \n",
      "\n",
      "Epoch  8\n",
      "-----\n",
      "Train Loss:  0.024308342608391806\tTest Loss:  0.9765817577519068\n",
      "Train Accuracy:  99.32898773006134% \tTest Accuracy:  78.0995934474759% \n",
      "\n",
      "Epoch  9\n",
      "-----\n",
      "Train Loss:  0.015354431275723994\tTest Loss:  1.063101846270445\n",
      "Train Accuracy:  99.5590490797546% \tTest Accuracy:  78.1504065525241% \n",
      "\n",
      "Epoch  10\n",
      "-----\n",
      "Train Loss:  0.011460602192952371\tTest Loss:  1.1404430517336217\n",
      "Train Accuracy:  99.65490797546013% \tTest Accuracy:  77.56605686211005% \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the training data\n",
    "# train_texts = [\"Example sentence 1\", \"Example sentence 2\", ...]\n",
    "# train_labels = [0, 1, ...]\n",
    "train_encodings = tokenizer(list(train_sarc_text_data), padding=True, truncation=True, max_length=128)\n",
    "test_encodings = tokenizer(list(test_sarc_text_data), padding=True, truncation=True, max_length=128)\n",
    "\n",
    "# Convert the data into PyTorch tensors\n",
    "train_dataset = torch.utils.data.TensorDataset(\n",
    "    torch.tensor(train_encodings['input_ids']),\n",
    "    torch.tensor(train_encodings['attention_mask']),\n",
    "    torch.tensor(list(train_sarc_class_data))\n",
    ")\n",
    "\n",
    "test_dataset = torch.utils.data.TensorDataset(\n",
    "    torch.tensor(test_encodings['input_ids']),\n",
    "    torch.tensor(test_encodings['attention_mask']),\n",
    "    torch.tensor(list(test_sarc_class_data))\n",
    ")\n",
    "\n",
    "\n",
    "# Define the training parameters\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=True)\n",
    "metric = BinaryAccuracy().to(device)\n",
    "\n",
    "\n",
    "# Train the BERT classifier\n",
    "classifier.train()\n",
    "for epoch in range(10):\n",
    "    print(\"Epoch \", epoch+1)\n",
    "    train_losses = []\n",
    "    train_acc = []\n",
    "    test_losses = []\n",
    "    test_acc = []\n",
    "    for idx, batch in enumerate(train_loader):\n",
    "        # batch.to(device)\n",
    "        if(idx % 40 == 0):\n",
    "            print(\"-\", end=\"\")\n",
    "        optimizer.zero_grad()\n",
    "        outputs = classifier(input_ids=batch[0].to(device), attention_mask=batch[1].to(device))\n",
    "        loss = criterion(outputs, batch[2].to(device))\n",
    "        loss.backward()\n",
    "        train_losses.append(loss.cpu().detach().numpy())\n",
    "        optimizer.step()\n",
    "        op = torch.argmax(outputs, axis = 1)\n",
    "        train_acc.append(metric(op, batch[2].to(device)).cpu().detach().numpy())\n",
    "        \n",
    "    for idx, batch in enumerate(test_loader):\n",
    "        # batch.to(device)\n",
    "        outputs = classifier(input_ids=batch[0].to(device), attention_mask=batch[1].to(device))\n",
    "        loss = criterion(outputs, batch[2].to(device))\n",
    "        test_losses.append(loss.cpu().detach().numpy())\n",
    "        op = torch.argmax(outputs, axis = 1)\n",
    "        test_acc.append(metric(op, batch[2].to(device)).cpu().detach().numpy())\n",
    "        \n",
    "        \n",
    "    print()\n",
    "    print(\"Train Loss: \",sum(train_losses) / len(train_losses), end = \"\\t\")\n",
    "    print(\"Test Loss: \",sum(test_losses) / len(test_losses), end = \"\\n\")   \n",
    "    print(\"Train Accuracy: \",(sum(train_acc) / len(train_acc)) * 100, end = \"% \\t\")\n",
    "    print(\"Test Accuracy: \",(sum(test_acc) / len(test_acc)) * 100, end = \"% \\n\")   \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "22ca1506-6955-43f8-b51a-7e0d5e838de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(classifier, \"BERTTransferHumor.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d948a6df-3897-407f-a0bf-0c5c89816d44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Chintan",
   "language": "python",
   "name": "chintan"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
