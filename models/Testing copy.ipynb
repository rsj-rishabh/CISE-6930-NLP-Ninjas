{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "acbff53c-ccea-4f26-b901-24e589aee6f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "      <th>author</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>score</th>\n",
       "      <th>ups</th>\n",
       "      <th>downs</th>\n",
       "      <th>date</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>parent_comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Bushie the other Bushie the next Bushie the af...</td>\n",
       "      <td>selicos</td>\n",
       "      <td>politics</td>\n",
       "      <td>606</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-10</td>\n",
       "      <td>2016-10-13 02:39:41</td>\n",
       "      <td>Don't you mean, Bushie the Younger?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Congrats on getting such damage before they we...</td>\n",
       "      <td>SenGenketsu</td>\n",
       "      <td>Robocraft</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-10</td>\n",
       "      <td>2016-10-27 16:51:51</td>\n",
       "      <td>So, Flak will be a thing again? Than i have to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Keeper of the Grove too</td>\n",
       "      <td>pucykoks</td>\n",
       "      <td>hearthstone</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-09</td>\n",
       "      <td>2016-09-13 21:33:07</td>\n",
       "      <td>Especially when you compare it to dalaran Mage...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>'This would be like my form of alcoholism' cle...</td>\n",
       "      <td>redditzendave</td>\n",
       "      <td>politics</td>\n",
       "      <td>7</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-10</td>\n",
       "      <td>2016-10-21 19:16:31</td>\n",
       "      <td>See It: Trump hires teen because she's beautif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Great input</td>\n",
       "      <td>Karieo</td>\n",
       "      <td>wow</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-09</td>\n",
       "      <td>2016-09-19 20:54:15</td>\n",
       "      <td>you swap to druid.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>0</td>\n",
       "      <td>Alright lmao, a bit *too* gay for my tastes</td>\n",
       "      <td>ScootaliciousScooter</td>\n",
       "      <td>teenagers</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-12</td>\n",
       "      <td>2016-12-30 04:05:43</td>\n",
       "      <td>This is just getting too far, sort of like you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>1</td>\n",
       "      <td>Rip this is the farthest I've ever gotten</td>\n",
       "      <td>EmceeSexy</td>\n",
       "      <td>teenagers</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-12</td>\n",
       "      <td>2016-12-05 02:18:19</td>\n",
       "      <td>ok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>1</td>\n",
       "      <td>Was expecting a tutorial on how to doxx your d...</td>\n",
       "      <td>Orrangejuiced</td>\n",
       "      <td>h1z1</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-12</td>\n",
       "      <td>2016-12-07 04:48:41</td>\n",
       "      <td>Ninja tutorial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>1</td>\n",
       "      <td>He just wants more time to prepare for playing...</td>\n",
       "      <td>bamachine</td>\n",
       "      <td>CFB</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-11</td>\n",
       "      <td>2016-11-23 03:55:23</td>\n",
       "      <td>Coaches Poll: Most feel Jimbo Fisher leaves fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>0</td>\n",
       "      <td>shit I am, well it isnt usual for me to see pe...</td>\n",
       "      <td>SFM_dude</td>\n",
       "      <td>StardustCrusaders</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-09</td>\n",
       "      <td>2016-09-20 17:43:20</td>\n",
       "      <td>Because I think you're confusing Peter Pan wit...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       label                                            comment  \\\n",
       "0          0  Bushie the other Bushie the next Bushie the af...   \n",
       "1          1  Congrats on getting such damage before they we...   \n",
       "2          0                            Keeper of the Grove too   \n",
       "3          1  'This would be like my form of alcoholism' cle...   \n",
       "4          1                                        Great input   \n",
       "...      ...                                                ...   \n",
       "99995      0        Alright lmao, a bit *too* gay for my tastes   \n",
       "99996      1          Rip this is the farthest I've ever gotten   \n",
       "99997      1  Was expecting a tutorial on how to doxx your d...   \n",
       "99998      1  He just wants more time to prepare for playing...   \n",
       "99999      0  shit I am, well it isnt usual for me to see pe...   \n",
       "\n",
       "                     author          subreddit  score  ups  downs     date  \\\n",
       "0                   selicos           politics    606   -1     -1  2016-10   \n",
       "1               SenGenketsu          Robocraft      1   -1     -1  2016-10   \n",
       "2                  pucykoks        hearthstone     14   14      0  2016-09   \n",
       "3             redditzendave           politics      7   -1     -1  2016-10   \n",
       "4                    Karieo                wow      2    2      0  2016-09   \n",
       "...                     ...                ...    ...  ...    ...      ...   \n",
       "99995  ScootaliciousScooter          teenagers      2   -1     -1  2016-12   \n",
       "99996             EmceeSexy          teenagers      1   -1     -1  2016-12   \n",
       "99997         Orrangejuiced               h1z1      3   -1     -1  2016-12   \n",
       "99998             bamachine                CFB      2   -1     -1  2016-11   \n",
       "99999              SFM_dude  StardustCrusaders      6    6      0  2016-09   \n",
       "\n",
       "               created_utc                                     parent_comment  \n",
       "0      2016-10-13 02:39:41                Don't you mean, Bushie the Younger?  \n",
       "1      2016-10-27 16:51:51  So, Flak will be a thing again? Than i have to...  \n",
       "2      2016-09-13 21:33:07  Especially when you compare it to dalaran Mage...  \n",
       "3      2016-10-21 19:16:31  See It: Trump hires teen because she's beautif...  \n",
       "4      2016-09-19 20:54:15                                 you swap to druid.  \n",
       "...                    ...                                                ...  \n",
       "99995  2016-12-30 04:05:43  This is just getting too far, sort of like you...  \n",
       "99996  2016-12-05 02:18:19                                                 ok  \n",
       "99997  2016-12-07 04:48:41                                     Ninja tutorial  \n",
       "99998  2016-11-23 03:55:23  Coaches Poll: Most feel Jimbo Fisher leaves fo...  \n",
       "99999  2016-09-20 17:43:20  Because I think you're confusing Peter Pan wit...  \n",
       "\n",
       "[100000 rows x 10 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# import os\n",
    "# import shutil\n",
    "\n",
    "# import tensorflow as tf\n",
    "# import tensorflow_hub as hub\n",
    "# import tensorflow_text as text\n",
    "# from official.nlp import optimization  # to create AdamW optimizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv(r'sarcasm_subset.csv')\n",
    "frames = [df]\n",
    "full = pd.concat(frames)\n",
    "full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4c126e9b-2e25-40e3-9e9c-9ff558f087e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = list(df[\"comment\"])[:5000]\n",
    "class_data = df[\"label\"].to_numpy()[:5000]\n",
    "for i in range(len(class_data)):\n",
    "    if class_data[i]=='notsarc':\n",
    "        class_data[i]=0\n",
    "    else:\n",
    "        class_data[i]=1\n",
    "        \n",
    "# for i in text_data:\n",
    "#     print(type(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4b56b027-6f26-4286-85b7-0f1a60d2709b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of BERT:  71.7 %\n",
      "F1 Score of BERT:  83.25725877360934 %\n",
      "\n",
      "Accuracy of BERT Transfer IMDB:  74.83999999999999 %\n",
      "F1 Score of BERT Transfer IMDB:  85.42702179046194 %\n",
      "\n",
      "Accuracy of BERT Transfer Irony:  67.24 %\n",
      "F1 Score of BERT Transfer Irony:  80.15246133136141 %\n",
      "\n",
      "Accuracy of BERT Transfer Humor:  73.18 %\n",
      "F1 Score of BERT Transfer Humor:  84.21637229858689 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define the classification model\n",
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, bert_model, num_classes):\n",
    "        super(BertClassifier, self).__init__()\n",
    "        self.bert = bert_model\n",
    "        self.fc = nn.Linear(768, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs[1]\n",
    "        logits = self.fc(pooled_output)\n",
    "        return logits\n",
    "\n",
    "# Load the pre-trained BERT model and tokenizer\n",
    "def BertTestAccF1(modelpath, text):\n",
    "    model = torch.load(modelpath)\n",
    "    model.to(device)\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    # Load the training data\n",
    "    # train_texts = [\"Example sentence 1\", \"Example sentence 2\", ...]\n",
    "    # train_labels = [0, 1, ...]\n",
    "\n",
    "    # Tokenize the text data\n",
    "    train_encodings = tokenizer(list(text_data), padding=True, truncation=True, max_length=128)\n",
    "    train_dataset = torch.utils.data.TensorDataset(\n",
    "        torch.tensor(train_encodings['input_ids']),\n",
    "        torch.tensor(train_encodings['attention_mask']),\n",
    "        torch.tensor(list(class_data))\n",
    "    )\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    f1_metric = BinaryF1Score().to(device)\n",
    "\n",
    "    acc = []\n",
    "    f1 = []\n",
    "    for idx, batch in enumerate(train_loader):\n",
    "        outputs = model(input_ids=batch[0].to(device), attention_mask=batch[1].to(device))\n",
    "        outputs = torch.argmax(outputs, axis = 1)\n",
    "        acc.append((outputs.cpu() == batch[2]).float().sum().numpy())\n",
    "        f1.append(f1_metric(outputs, batch[2].to(device)).cpu().detach().numpy())\n",
    "\n",
    "    print(f\"Accuracy of {text}: \", (sum(acc) / 5000) * 100, \"%\")\n",
    "    print(f\"F1 Score of {text}: \", (sum(f1) / len(f1)) * 100, \"%\")\n",
    "    print()\n",
    "    \n",
    "    \n",
    "BertTestAccF1(\"models/BERT.pth\", \"BERT\")\n",
    "BertTestAccF1(\"models/BERTTransfer.pth\", \"BERT Transfer IMDB\")\n",
    "BertTestAccF1(\"models/BERTTransferIrony.pth\", \"BERT Transfer Irony\")\n",
    "BertTestAccF1(\"models/BERTTransferHumor.pth\", \"BERT Transfer Humor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0940df5e-b29c-47d7-90f0-25b5db5edd8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of RoBERTa:  66.64 %\n",
      "F1 Score of RoBERTa:  79.76926238673508 %\n",
      "\n",
      "Accuracy of RoBERTa Transfer IMDB:  72.88 %\n",
      "F1 Score of RoBERTa Transfer IMDB:  84.0859683455935 %\n",
      "\n",
      "Accuracy of RoBERTa Transfer Irony:  64.64 %\n",
      "F1 Score of RoBERTa Transfer Irony:  78.24215167646955 %\n",
      "\n",
      "Accuracy of RoBERTa Transfer Humor:  71.86 %\n",
      "F1 Score of RoBERTa Transfer Humor:  83.37917430385663 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torchmetrics.classification import BinaryF1Score, BinaryPrecisionRecallCurve\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# Define the classification model\n",
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, bert_model, num_classes):\n",
    "        super(BertClassifier, self).__init__()\n",
    "        self.bert = bert_model\n",
    "        self.fc = nn.Linear(768, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs[1]\n",
    "        logits = self.fc(pooled_output)\n",
    "        return logits\n",
    "\n",
    "# Load the pre-trained BERT model and tokenizer\n",
    "def RoBertaTestAccF1(modelpath, text):\n",
    "    model = torch.load(modelpath)\n",
    "    model.to(device)\n",
    "    tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "    # Load the training data\n",
    "    # train_texts = [\"Example sentence 1\", \"Example sentence 2\", ...]\n",
    "    # train_labels = [0, 1, ...]\n",
    "\n",
    "    # Tokenize the text data\n",
    "    train_encodings = tokenizer(list(text_data), padding=True, truncation=True, max_length=128)\n",
    "    train_dataset = torch.utils.data.TensorDataset(\n",
    "        torch.tensor(train_encodings['input_ids']),\n",
    "        torch.tensor(train_encodings['attention_mask']),\n",
    "        torch.tensor(list(class_data))\n",
    "    )\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    f1_metric = BinaryF1Score().to(device)\n",
    "\n",
    "    acc = []\n",
    "    f1 = []\n",
    "    for idx, batch in enumerate(train_loader):\n",
    "        outputs = model(input_ids=batch[0].to(device), attention_mask=batch[1].to(device))\n",
    "        outputs = torch.argmax(outputs, axis = 1)\n",
    "        acc.append((outputs.cpu() == batch[2]).float().sum().numpy())\n",
    "        f1.append(f1_metric(outputs, batch[2].to(device)).cpu().detach().numpy())\n",
    "\n",
    "    print(f\"Accuracy of {text}: \", (sum(acc) / 5000) * 100, \"%\")\n",
    "    print(f\"F1 Score of {text}: \", (sum(f1) / len(f1)) * 100, \"%\")\n",
    "    print()\n",
    "    \n",
    "    \n",
    "RoBertaTestAccF1(\"models/RoBERTa.pth\", \"RoBERTa\")\n",
    "RoBertaTestAccF1(\"models/RoBERTaTransfer.pth\", \"RoBERTa Transfer IMDB\")\n",
    "RoBertaTestAccF1(\"models/RoBERTaTransferIrony.pth\", \"RoBERTa Transfer Irony\")\n",
    "RoBertaTestAccF1(\"models/RoBERTaTransferHumor.pth\", \"RoBERTa Transfer Humor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "911e19c4-9d2d-45ec-95e9-21123ee2e96d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score of BERT:  : 80.15630441467937\n",
      "F1 Score of RoBERTa:  : 83.8939377447454\n",
      "F1 Score of BERT Transfer IMDB:  : 79.46329567490554\n",
      "F1 Score of RoBERTa Transfer IMDB:  : 82.4882546576058\n",
      "F1 Score of BERT Transfer Irony:  : 77.55576052316805\n",
      "F1 Score of RoBERTa Transfer Irony:  : 82.20451910321306\n",
      "F1 Score of BERT Transfer Humor:  : 78.69429515629281\n",
      "F1 Score of RoBERTa Transfer Humor:  : 82.35881212281018\n"
     ]
    }
   ],
   "source": [
    "from torchmetrics.classification import BinaryF1Score, BinaryPrecisionRecallCurve\n",
    "from transformers import AutoTokenizer, AutoModel, BertModel, BertTokenizer \n",
    "\n",
    "# Define the classification model\n",
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, bert_model, num_classes):\n",
    "        super(BertClassifier, self).__init__()\n",
    "        self.bert = bert_model\n",
    "        self.fc = nn.Linear(768, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs[1]\n",
    "        logits = self.fc(pooled_output)\n",
    "        return logits\n",
    "\n",
    "# Load the pre-trained BERT model and tokenizer\n",
    "model_1 = torch.load('models/BERT.pth')\n",
    "model_2 = torch.load('models/RoBERTa.pth')\n",
    "model_3 = torch.load('models/BERTTransfer.pth')\n",
    "model_4 = torch.load('models/RoBERTaTransfer.pth')\n",
    "model_5 = torch.load('models/BERTTransferIrony.pth')\n",
    "model_6 = torch.load('models/RoBERTaTransferIrony.pth')\n",
    "model_7 = torch.load('models/BERTTransferHumor.pth')\n",
    "model_8 = torch.load('models/RoBERTaTransferHumor.pth')\n",
    "\n",
    "model.to(device)\n",
    "roberta_tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_test_encodings = bert_tokenizer(list(test_sarc_text_data), padding=True, truncation=True, max_length=128)\n",
    "roberta_test_encodings = roberta_tokenizer(list(test_sarc_text_data), padding=True, truncation=True, max_length=128)\n",
    "\n",
    "bert_test_dataset = torch.utils.data.TensorDataset(\n",
    "    torch.tensor(bert_test_encodings['input_ids']),\n",
    "    torch.tensor(bert_test_encodings['attention_mask']),\n",
    "    torch.tensor(list(test_sarc_class_data))\n",
    ")\n",
    "\n",
    "\n",
    "roberta_test_dataset = torch.utils.data.TensorDataset(\n",
    "    torch.tensor(roberta_test_encodings['input_ids']),\n",
    "    torch.tensor(roberta_test_encodings['attention_mask']),\n",
    "    torch.tensor(list(test_sarc_class_data))\n",
    ")\n",
    "\n",
    "bert_test_loader = torch.utils.data.DataLoader(bert_test_dataset, batch_size=32, shuffle=True)\n",
    "roberta_test_loader = torch.utils.data.DataLoader(roberta_test_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "def calculate_metrics(model, loader):\n",
    "    f1 = []\n",
    "    precision = []\n",
    "    recall = []\n",
    "    \n",
    "    f1_metric = BinaryF1Score().to(device)\n",
    "    p_r = BinaryPrecisionRecallCurve().to(device)\n",
    "    \n",
    "    for idx, batch in enumerate(loader):\n",
    "        # batch.to(device)\n",
    "        outputs = model(input_ids=batch[0].to(device), attention_mask=batch[1].to(device))\n",
    "        op = torch.argmax(outputs, axis = 1)\n",
    "        \n",
    "        f1.append(f1_metric(op, batch[2].to(device)).cpu().detach().numpy())\n",
    "        # print(p_r(op.float(), batch[2].to(device)))\n",
    "        # print(p)\n",
    "        # print(r)\n",
    "        \n",
    "        # precision.append(p.cpu().detach().numpy())\n",
    "        # recall.append(r.cpu().detach().numpy())\n",
    "    \n",
    "    return (sum(f1) / len(f1)) * 100\n",
    "\n",
    "\n",
    "def print_metrics(model, loader, text):\n",
    "    f1 = calculate_metrics(model, loader)\n",
    "    print(f\"F1 Score of {text} : {f1}\")\n",
    "    \n",
    "    \n",
    "\n",
    "print_metrics(model_1, bert_test_loader, \"BERT: \")\n",
    "print_metrics(model_2, roberta_test_loader, \"RoBERTa: \")\n",
    "print_metrics(model_3, bert_test_loader, \"BERT Transfer IMDB: \")\n",
    "print_metrics(model_4, roberta_test_loader, \"RoBERTa Transfer IMDB: \")\n",
    "\n",
    "print_metrics(model_5, bert_test_loader, \"BERT Transfer Irony: \")\n",
    "print_metrics(model_6, roberta_test_loader, \"RoBERTa Transfer Irony: \")\n",
    "print_metrics(model_7, bert_test_loader, \"BERT Transfer Humor: \")\n",
    "print_metrics(model_8, roberta_test_loader, \"RoBERTa Transfer Humor: \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee53c2b-53b9-4ffa-8854-72231b1c0d1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5fa8a8-d46a-4b78-829b-72ccfec90727",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Chintan",
   "language": "python",
   "name": "chintan"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
