{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acbff53c-ccea-4f26-b901-24e589aee6f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>If that's true, then Freedom of Speech is doom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Neener neener - is it time to go in from the p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Just like the plastic gun fear, the armour pie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>So geology is a religion because we weren't he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>Well done Monty. Mark that up as your first ev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6515</th>\n",
       "      <td>1</td>\n",
       "      <td>6516</td>\n",
       "      <td>depends on when the baby bird died.   run alon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6516</th>\n",
       "      <td>1</td>\n",
       "      <td>6517</td>\n",
       "      <td>ok, sheesh, to clarify, women who arent aborti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6517</th>\n",
       "      <td>1</td>\n",
       "      <td>6518</td>\n",
       "      <td>so..   eh??   hows this sound?   will it fly w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6518</th>\n",
       "      <td>1</td>\n",
       "      <td>6519</td>\n",
       "      <td>I think we should put to a vote, the right of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6519</th>\n",
       "      <td>1</td>\n",
       "      <td>6520</td>\n",
       "      <td>You have a blob of tissue in your \"but(sic)\"? ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6520 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     class    id                                               text\n",
       "0        0     1  If that's true, then Freedom of Speech is doom...\n",
       "1        0     2  Neener neener - is it time to go in from the p...\n",
       "2        0     3  Just like the plastic gun fear, the armour pie...\n",
       "3        0     4  So geology is a religion because we weren't he...\n",
       "4        0     5  Well done Monty. Mark that up as your first ev...\n",
       "...    ...   ...                                                ...\n",
       "6515     1  6516  depends on when the baby bird died.   run alon...\n",
       "6516     1  6517  ok, sheesh, to clarify, women who arent aborti...\n",
       "6517     1  6518  so..   eh??   hows this sound?   will it fly w...\n",
       "6518     1  6519  I think we should put to a vote, the right of ...\n",
       "6519     1  6520  You have a blob of tissue in your \"but(sic)\"? ...\n",
       "\n",
       "[6520 rows x 3 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(r'gen_dataset.csv')\n",
    "text_data = list(df[\"text\"])\n",
    "class_data = df[\"class\"].to_numpy()\n",
    "for i in range(len(class_data)):\n",
    "    if class_data[i]=='notsarc':\n",
    "        class_data[i]=0\n",
    "    else:\n",
    "        class_data[i]=1\n",
    "class_data = list(class_data)\n",
    "train_sarc_text_data = text_data[:int(len(text_data) * 0.8)]\n",
    "test_sarc_text_data = text_data[int(len(text_data) * 0.8):]\n",
    "\n",
    "train_sarc_class_data = class_data[:int(len(class_data) * 0.8)]\n",
    "test_sarc_class_data = class_data[int(len(class_data) * 0.8):]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b56b027-6f26-4286-85b7-0f1a60d2709b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torchmetrics.classification import BinaryAccuracy\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Load the pre-trained BERT model and tokenizer\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "model.to(device)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Define the classification model\n",
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, bert_model, num_classes):\n",
    "        super(BertClassifier, self).__init__()\n",
    "        self.bert = bert_model\n",
    "        self.fc = nn.Linear(768, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs[1]\n",
    "        logits = self.fc(pooled_output)\n",
    "        return logits\n",
    "\n",
    "\n",
    "# Define the training parameters\n",
    "classifier = BertClassifier(model, num_classes=2)\n",
    "classifier.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=2e-5)\n",
    "metric = BinaryAccuracy().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d54c6def-01e2-4ef4-9179-354abd5fb53b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1\n",
      "-----\n",
      "Train Loss:  0.5095769826071394\tTest Loss:  0.5052340539490304\n",
      "Train Accuracy:  75.53680981595092% \tTest Accuracy:  74.59349588650029% \n",
      "\n",
      "Epoch  2\n",
      "-----\n",
      "Train Loss:  0.3863767779495087\tTest Loss:  0.4494323061733711\n",
      "Train Accuracy:  83.22469325153375% \tTest Accuracy:  79.75101630862166% \n",
      "\n",
      "Epoch  3\n",
      "-----\n",
      "Train Loss:  0.23656198994879343\tTest Loss:  0.5316118190928203\n",
      "Train Accuracy:  91.25766871165644% \tTest Accuracy:  79.2174797232558% \n",
      "\n",
      "Epoch  4\n",
      "-----\n",
      "Train Loss:  0.12748808382890342\tTest Loss:  0.6602444116298746\n",
      "Train Accuracy:  95.954754601227% \tTest Accuracy:  76.65142271576858% \n",
      "\n",
      "Epoch  5\n",
      "-----\n",
      "Train Loss:  0.06820839633765213\tTest Loss:  0.7532000327255668\n",
      "Train Accuracy:  98.29371165644172% \tTest Accuracy:  77.84552850374361% \n",
      "\n",
      "Epoch  6\n",
      "-----\n",
      "Train Loss:  0.0542087491453151\tTest Loss:  0.7763065877484112\n",
      "Train Accuracy:  98.56211656441718% \tTest Accuracy:  79.62398369137834% \n",
      "\n",
      "Epoch  7\n",
      "-----\n",
      "Train Loss:  0.04143621330577994\tTest Loss:  0.9051397319247083\n",
      "Train Accuracy:  98.81134969325154% \tTest Accuracy:  78.125% \n",
      "\n",
      "Epoch  8\n",
      "-----\n",
      "Train Loss:  0.037744599445922976\tTest Loss:  0.8305375786816201\n",
      "Train Accuracy:  98.88803680981594% \tTest Accuracy:  80.23373978893932% \n",
      "\n",
      "Epoch  9\n",
      "-----\n",
      "Train Loss:  0.019501750386731437\tTest Loss:  0.9534094580789891\n",
      "Train Accuracy:  99.5398773006135% \tTest Accuracy:  78.58231707317073% \n",
      "\n",
      "Epoch  10\n",
      "-----\n",
      "Train Loss:  0.01859969502112091\tTest Loss:  0.9096878061934215\n",
      "Train Accuracy:  99.5207055214724% \tTest Accuracy:  80.46239832552467% \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the training data\n",
    "# train_texts = [\"Example sentence 1\", \"Example sentence 2\", ...]\n",
    "# train_labels = [0, 1, ...]\n",
    "\n",
    "# Tokenize the text data\n",
    "train_encodings = tokenizer(list(train_sarc_text_data), padding=True, truncation=True, max_length=128)\n",
    "test_encodings = tokenizer(list(test_sarc_text_data), padding=True, truncation=True, max_length=128)\n",
    "\n",
    "# Convert the data into PyTorch tensors\n",
    "train_dataset = torch.utils.data.TensorDataset(\n",
    "    torch.tensor(train_encodings['input_ids']),\n",
    "    torch.tensor(train_encodings['attention_mask']),\n",
    "    torch.tensor(list(train_sarc_class_data))\n",
    ")\n",
    "\n",
    "test_dataset = torch.utils.data.TensorDataset(\n",
    "    torch.tensor(test_encodings['input_ids']),\n",
    "    torch.tensor(test_encodings['attention_mask']),\n",
    "    torch.tensor(list(test_sarc_class_data))\n",
    ")\n",
    "\n",
    "\n",
    "# Define the training parameters\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=True)\n",
    "metric = BinaryAccuracy().to(device)\n",
    "\n",
    "\n",
    "# Train the BERT classifier\n",
    "classifier.train()\n",
    "for epoch in range(10):\n",
    "    print(\"Epoch \", epoch+1)\n",
    "    train_losses = []\n",
    "    train_acc = []\n",
    "    test_losses = []\n",
    "    test_acc = []\n",
    "    for idx, batch in enumerate(train_loader):\n",
    "        # batch.to(device)\n",
    "        if(idx % 40 == 0):\n",
    "            print(\"-\", end=\"\")\n",
    "        optimizer.zero_grad()\n",
    "        outputs = classifier(input_ids=batch[0].to(device), attention_mask=batch[1].to(device))\n",
    "        loss = criterion(outputs, batch[2].to(device))\n",
    "        loss.backward()\n",
    "        train_losses.append(loss.cpu().detach().numpy())\n",
    "        optimizer.step()\n",
    "        op = torch.argmax(outputs, axis = 1)\n",
    "        train_acc.append(metric(op, batch[2].to(device)).cpu().detach().numpy())\n",
    "        \n",
    "    for idx, batch in enumerate(test_loader):\n",
    "        # batch.to(device)\n",
    "        outputs = classifier(input_ids=batch[0].to(device), attention_mask=batch[1].to(device))\n",
    "        loss = criterion(outputs, batch[2].to(device))\n",
    "        test_losses.append(loss.cpu().detach().numpy())\n",
    "        op = torch.argmax(outputs, axis = 1)\n",
    "        test_acc.append(metric(op, batch[2].to(device)).cpu().detach().numpy())\n",
    "        \n",
    "        \n",
    "    print()\n",
    "    print(\"Train Loss: \",sum(train_losses) / len(train_losses), end = \"\\t\")\n",
    "    print(\"Test Loss: \",sum(test_losses) / len(test_losses), end = \"\\n\")   \n",
    "    print(\"Train Accuracy: \",(sum(train_acc) / len(train_acc)) * 100, end = \"% \\t\")\n",
    "    print(\"Test Accuracy: \",(sum(test_acc) / len(test_acc)) * 100, end = \"% \\n\")   \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22ca1506-6955-43f8-b51a-7e0d5e838de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(classifier, \"BERT.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d948a6df-3897-407f-a0bf-0c5c89816d44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Chintan",
   "language": "python",
   "name": "chintan"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
