{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acbff53c-ccea-4f26-b901-24e589aee6f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        class    id                                               text\n",
      "0     notsarc     1  If that's true, then Freedom of Speech is doom...\n",
      "1     notsarc     2  Neener neener - is it time to go in from the p...\n",
      "2     notsarc     3  Just like the plastic gun fear, the armour pie...\n",
      "3     notsarc     4  So geology is a religion because we weren't he...\n",
      "4     notsarc     5  Well done Monty. Mark that up as your first ev...\n",
      "...       ...   ...                                                ...\n",
      "6515     sarc  6516  depends on when the baby bird died.   run alon...\n",
      "6516     sarc  6517  ok, sheesh, to clarify, women who arent aborti...\n",
      "6517     sarc  6518  so..   eh??   hows this sound?   will it fly w...\n",
      "6518     sarc  6519  I think we should put to a vote, the right of ...\n",
      "6519     sarc  6520  You have a blob of tissue in your \"but(sic)\"? ...\n",
      "\n",
      "[6520 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(r'gen_dataset.csv')\n",
    "print(df)\n",
    "text_data = list(df[\"text\"])\n",
    "class_data = df[\"class\"].to_numpy()\n",
    "for i in range(len(class_data)):\n",
    "    if class_data[i]=='notsarc':\n",
    "        class_data[i]=0\n",
    "    else:\n",
    "        class_data[i]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5771c3ce-a814-4afc-9863-7587f4790414",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_data = list(class_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b56b027-6f26-4286-85b7-0f1a60d2709b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1\n",
      "-----------\n",
      "0.511029992471723\n",
      "Epoch  2\n",
      "-----------\n",
      "0.37196221414442154\n",
      "Epoch  3\n",
      "-----------\n",
      "0.21462083403386323\n",
      "Epoch  4\n",
      "-----------\n",
      "0.10037050861865282\n",
      "Epoch  5\n",
      "-----------\n",
      "0.05008825825015996\n",
      "Epoch  6\n",
      "-----------\n",
      "0.036462544384823346\n",
      "Epoch  7\n",
      "-----------\n",
      "0.03009487843019979\n",
      "Epoch  8\n",
      "-----------\n",
      "0.021774021621026537\n",
      "Epoch  9\n",
      "-----------\n",
      "0.017147237602472946\n",
      "Epoch  10\n",
      "-----------\n",
      "0.017462997331783886\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Load the pre-trained BERT model and tokenizer\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "model.to(device)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Define the classification model\n",
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, bert_model, num_classes):\n",
    "        super(BertClassifier, self).__init__()\n",
    "        self.bert = bert_model\n",
    "        self.fc = nn.Linear(768, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs[1]\n",
    "        logits = self.fc(pooled_output)\n",
    "        return logits\n",
    "\n",
    "# Load the training data\n",
    "# train_texts = [\"Example sentence 1\", \"Example sentence 2\", ...]\n",
    "# train_labels = [0, 1, ...]\n",
    "\n",
    "# Tokenize the text data\n",
    "train_encodings = tokenizer(list(text_data), padding=True, truncation=True, max_length=128)\n",
    "\n",
    "# Convert the data into PyTorch tensors\n",
    "train_dataset = torch.utils.data.TensorDataset(\n",
    "    torch.tensor(train_encodings['input_ids']),\n",
    "    torch.tensor(train_encodings['attention_mask']),\n",
    "    torch.tensor(list(class_data))\n",
    ")\n",
    "\n",
    "# Define the training parameters\n",
    "classifier = BertClassifier(model, num_classes=2)\n",
    "classifier.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=2e-5)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Train the BERT classifier\n",
    "classifier.train()\n",
    "for epoch in range(10):\n",
    "    print(\"Epoch \", epoch+1)\n",
    "    losses = []\n",
    "    for idx, batch in enumerate(train_loader):\n",
    "        # batch.to(device)\n",
    "        if(idx % 20 == 0):\n",
    "            print(\"-\", end=\"\")\n",
    "        optimizer.zero_grad()\n",
    "        outputs = classifier(input_ids=batch[0].to(device), attention_mask=batch[1].to(device))\n",
    "        loss = criterion(outputs, batch[2].to(device))\n",
    "        loss.backward()\n",
    "        losses.append(loss.cpu().detach().numpy())\n",
    "        optimizer.step()\n",
    "    print()\n",
    "    print(sum(losses) / len(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e11d07-a92f-4382-b814-1ab2e094f771",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = clas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54c6def-01e2-4ef4-9179-354abd5fb53b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch-1.8.1",
   "language": "python",
   "name": "pytorch-1.8.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
